Personal introduction:
Hi everyone, my name is Alisa and today I wanted to present you one of the data science topics. But first a litle bit about me: I have studied economics and mathematics as well as economics and informatics. I have experience in backend engineering, data engineering and data science. Through all my professional life I enjoyed working on data powered projects be it an algorythm itself or a fuzz aroud. You probably all heard or encountered really cool and state of the art machine learning projects with an astounishing performance. Thave you actually thought what it took to get these results? It is actually quite a long and a delicate process. I personally am more fascinated be the development process rather than by the end result of such projects. This is why I wanted to share my experience with feature engineering that plays a crucial role in a models success.

Presentation Intro:
In this presentation I wanted to talk about a situation when you do need to use some machine learning algorithm but do not have a massive amount of a high quality data. By the way, by machine learning I will mean pretty much everything more complex than a regular expression. And yeah, I am aware, that regex can be very annoying and complex +) 
The talk will consist of a brief introductinary section on when you actually should consider using machine learning, what problems you should consider before the final descision and what you should be aware of when decided on goind deep. I will, of course, say several words on what feature engineering actually is.
I will show 2 different examples of feature engineering in practice.The first one is a binary classification with a tiny neural network. The second one is  a regression with a random forest model. I will not provide explanation on what those models are and how they work since it’s a bit out of the scope of the presentation. All the information on this and others matters can be found at the end of the slides.
As a conclusion I will again talk about when feature engineering makes sense and what alternatives there are.

As I already mentioned we will start with a brief discussion wehteher ml is actually needed.

When to use ML:
Before jumping into data collection and model development you need to decide what do you actually want to do. What is your product? What should the end-user get? The relevant question for this presentation is what place takes the smart algorithm in the whole development process? Is it in the very core of the product or even the produt itself? For example Smart crawler or a medical support system would be a good example of ML as a service. Next level where you still should spend time on setting everything up for using ML is a core feature of the product that distinguishes you from the competition. As an example I would all a Groceries list that automatically calculates the total macronutritions and gives you a recommendation to change something in the list. Another sophisticated thing would be a todo list that balances the order of the tasks on several important criterias. But if you focus on a certain product that can be launched without the absolut need for sophisticated needs, than you probaby should concentrate on rounding up the product first and ony afterwards expand on the market by adding some mind blowing features.
Another important thing to keep in mind: depending on the stage the product or the company is currently being influences the descision to invest heavily into ml. Generally, if the algorithm is not a core product and the company haven’t stabilized the market share and financial position, investments into sophisticated data science should be avoided or very carefully estimated, postponed and estimated again. At the end, quite a lot of things can be faken or replaced with a human labour, at least for a test phase.

Potential problems and cost of setting up the ml instructure:
If you decide to use data in some sophisticated way for your project you need to consider several important things first: where the data is coming from, in what form do you may to store it, for how long, how do you may use it, whether you may open it to a wide publicity. After figuring out the legal issues with the main data flow you should set up a proper infrastructre. Normally so called data engineering team takes over this task: setting up the data collection services, communication pipes, prepare and maintain all possible databases as welll as the initial data precleaning. After the sane data is centrally available to the rest of the team the data scienctists come into the spotlight. 
This is the step where the magic starts to happen: data analysis, following by feature selection and model creation, testing, fine tuning and so on until having a satisfactory results. The final step that is required careful thoughts is insights usage. Ow to deploy the model, whether the model have to be retrained constantly, automated testing like a/b and golden standard testing. All of these should be taken into account before giving any further tought if the ml could be useful. Every step is associated with a large cost. If any detail is ignored , the team is being set up for a potential failure and a guaranteed dissatisfaction and unhappiness. 

Assume, you got further into the process and have prepared the infrastructure for the data science team. I just said that there is a risk to set up a team to fail if you miss or ignore some of the decision points. Well, I have a surprise for you, there are even more pitfalls coming! Normally data scientists are not someone who decies what the whole fuzz is about. By fuzz I mean some business value that should be produced at the end of the day. As the practice shows nobody knows better what could be valuble rather than business people, since they create and work with the business cases directly. So, at first, as a clear task shpuld be defined for the data science team. You should try to avoid certain extrems: data task is not an ordinary projects where the normal planning applies. After the task is discussed with the team and the details are more or less defined the QA comes into discussion. Not only the parameters of the model (Accuracy, Errors, R2, etc.) but in what form should a result be delivered. How can it be tested internally what is considered to be good enough to go onto production and into the testing with th real users. Also the results on the production can and will differ from the sandbox environmnet due to many assumptions and simplifictaions made during the whole development process. Also do not forget the cost of evry and each detail in every step. It all adds up to a quite impressive investment. After going through all the previous steps you data team takes the lead and actually have to check the quality and the quantity of the data aailable for an upcoming task.
Data quality: all further technical decisions depend 100% on what data you have. Are you in a big data game? The algorithms and the infrastructure shoud be adjusted accordignly. Do you work with a real time data streams or with a large historical data? Again, adjust every single step decision from before.Yyou don’t have that much data? What kind of data is it? Is it a high dimensional feature-rich data like a sound-recording or an image? Is it a full scientific paper with all the sources included? Or is it some data desribing an even of some sort? Usrer login, user purchase, working hours recorded? A piece of code maybe? Depending on what class of the data it is your choice of an algorihm et limited pretty drastically.

In this presentation I wll talk about a case where the data looks flat and is somewhat poor in numbers. In this case you really have to do some magic and brainstorming in order not to feed the model a garbage. So, I will start talking about what feature engineering is not: it should not be a part of the pre-cleaning or data collecion steps. During those stages the rule: the more the better applies. What feature engineering is: 
1. it is somewhat abstract concep hat is actually used in every data related sphere
2. you will have hard time finding formal information on this
3. This is expert domain knowledge
4. It can be both manual or automatic (feature lerning, but this one is very much out of the scope of this presentation)
5. The process is expensive fro both time and money prespectives
6. Proper features might bring more performace boost than switching around th models.
When people are talking aout machine learning or deep learning all it boils down to a predicting an outcome of even more abstartct getting a short answer to a question about certain event. No event is separated from the outside world, everything happens under certain cercumstances. We are not spherical hourses living in vacuum, which means the models to be able to perform should know as much as possible about the situation. Feature engineerig in other words is providing the model with some data it can not figure out on it’s own because of the missing context, missing data or algorithm itself.  My talk I will focus on feature engineering in low dimensional data. 

First example

Second example

Conclusion:
Feature engineering is dangerous, costl but very powerful if done right. 



